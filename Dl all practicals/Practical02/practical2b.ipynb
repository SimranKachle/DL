{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a91a8b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Embedding, SpatialDropout1D\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "909d192a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load and preprocess the dataset\n",
    "df = pd.read_csv('IMDB Dataset.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d415f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Map 'positive' to 1 and 'negative' to 0\n",
    "df['sentiment'] = df['sentiment'].map({'positive': 1, 'negative': 0})\n",
    "\n",
    "# Tokenize the text data\n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(df['review'])\n",
    "X = tokenizer.texts_to_sequences(df['review'])\n",
    "X = pad_sequences(X, padding='pre', maxlen=100)\n",
    "y = df['sentiment']\n",
    "\n",
    "# Tokenizer Initialization:\n",
    "# We create a Tokenizer object called tokenizer and specify that we want it to consider only the top 5000 most frequent words (num_words=5000).\n",
    "# Fitting the Tokenizer:\n",
    "# We provide the tokenizer with our review texts (df['review']), and it learns from them. Essentially, it creates a vocabulary of words from these reviews and assigns each word a unique number.\n",
    "# Tokenizing Sequences:\n",
    "# Now that the tokenizer knows the words and their corresponding numbers, we convert each review text into a sequence of numbers. Instead of words, each review is represented by a sequence of these numbers.\n",
    "# Padding Sequences:\n",
    "# To ensure that all sequences have the same length, we add padding to the beginning of each sequence if it's shorter than 100 numbers. This ensures that every review has the same length, which is necessary when training a neural network.\n",
    "# Assigning Labels:\n",
    "# We also extract the sentiment labels (positive or negative) from the DataFrame and assign them to y. This tells us whether each review is positive or negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "904b1104",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Define the deep neural network model architecture\n",
    "embedding_dim = 128\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=5000, output_dim=embedding_dim, input_length=100))\n",
    "model.add(SpatialDropout1D(0.2))\n",
    "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b4262a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "625/625 [==============================] - 208s 325ms/step - loss: 0.4030 - accuracy: 0.8148 - val_loss: 0.3134 - val_accuracy: 0.8660\n",
      "Epoch 2/5\n",
      "625/625 [==============================] - 238s 381ms/step - loss: 0.2914 - accuracy: 0.8775 - val_loss: 0.3094 - val_accuracy: 0.8649\n",
      "Epoch 3/5\n",
      "625/625 [==============================] - 282s 452ms/step - loss: 0.2481 - accuracy: 0.8989 - val_loss: 0.3025 - val_accuracy: 0.8695\n",
      "Epoch 4/5\n",
      "625/625 [==============================] - 230s 368ms/step - loss: 0.2162 - accuracy: 0.9130 - val_loss: 0.3270 - val_accuracy: 0.8694\n",
      "Epoch 5/5\n",
      "625/625 [==============================] - 241s 385ms/step - loss: 0.1904 - accuracy: 0.9248 - val_loss: 0.3378 - val_accuracy: 0.8648\n",
      "Training time: 1200.30 seconds\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Step 4: Train the model\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "start_time = time.time()\n",
    "model.fit(X_train, y_train, epochs=5, batch_size=64, validation_data=(X_test, y_test))\n",
    "end_time = time.time()\n",
    "training_time = end_time - start_time\n",
    "print(f\"Training time: {training_time:.2f} seconds\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "409c6573",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 16s 52ms/step - loss: 0.3378 - accuracy: 0.8648\n",
      "Test Loss: 0.3378496766090393\n",
      "Test Accuracy: 0.864799976348877\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f'Test Loss: {loss}')\n",
    "print(f'Test Accuracy: {accuracy}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7cf056",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a movie review: The leafs are dry \n",
      "1/1 [==============================] - 3s 3s/step\n",
      "Predicted sentiment: Negative\n",
      "Prediction time: 2.77 seconds\n",
      "Enter a movie review: Outstanding Acting by Randeep. Its great to see A Hero on screen. SwatantryaVeer Savarkar is like Epitome of Patriotism and Hinduness. He is fountain head of Hindutva Idelogy who runs India now and father of Intellectuallism in India. He is one icon of Dalit Movement. This movie shown all of this with good pace and direction. Randeeps acting is incredible and must to watch. Savarkar is from Land of Hindu King Shree Chatrapati Shivaji ,Maharaj. The movie deserve all the love of Patriot Indians. Must watch movie for everyone. Randeep, Ankita Lokhande all did oustading efforts to make this movie. Epic Tale of Heroism for The Nation.\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "Predicted sentiment: Positive\n",
      "Prediction time: 0.16 seconds\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Make predictions on new data (dynamic input)\n",
    "while True:\n",
    "    user_input = input(\"Enter a movie review: \")\n",
    "    # Preprocess the user input\n",
    "    input_sequence = tokenizer.texts_to_sequences([user_input])\n",
    "    input_sequence = pad_sequences(input_sequence, padding='pre', maxlen=100)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    # Make prediction\n",
    "    prediction = model.predict(input_sequence)[0][0]\n",
    "    end_time = time.time()\n",
    "    \n",
    "    # Print prediction\n",
    "    if prediction >= 0.5:\n",
    "        print(\"Predicted sentiment: Positive\")\n",
    "    else:\n",
    "        print(\"Predicted sentiment: Negative\")\n",
    "    \n",
    "    prediction_time = end_time - start_time\n",
    "    print(f\"Prediction time: {prediction_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d81a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sure, let's break down each layer in detail:\n",
    "\n",
    "# 1. **Embedding Layer**:\n",
    "#    - The `Embedding` layer is responsible for converting the sequences of numbers representing words into dense vectors of fixed size. It essentially learns to represent words in a continuous vector space where words with similar meanings have similar vector representations.\n",
    "#    - `input_dim=5000` specifies the size of the vocabulary, i.e., the number of unique words in our dataset.\n",
    "#    - `output_dim=embedding_dim=128` specifies the dimensionality of the dense embedding. Each word will be represented by a vector of size 128.\n",
    "#    - `input_length=100` specifies the length of the input sequences, i.e., the maximum length of a review after padding.\n",
    "\n",
    "# 2. **SpatialDropout1D Layer**:\n",
    "#    - The `SpatialDropout1D` layer applies dropout to the embeddings. Dropout is a regularization technique used to prevent overfitting by randomly setting a fraction of input units to zero during training. \n",
    "#    - `0.2` is the dropout rate, meaning that 20% of the elements in the embedding vectors will be randomly set to zero during each update.\n",
    "\n",
    "# 3. **LSTM Layer**:\n",
    "#    - The `LSTM` (Long Short-Term Memory) layer is a type of recurrent neural network (RNN) layer that is capable of learning long-term dependencies in sequence data.\n",
    "#    - `100` is the number of units (or cells) in the LSTM layer. This parameter controls the dimensionality of the output space.\n",
    "#    - `dropout=0.2` specifies the dropout rate for the input units of the LSTM layer. It means that 20% of the input units will be dropped during training.\n",
    "#    - `recurrent_dropout=0.2` specifies the dropout rate for the recurrent units (the connections between the cells) of the LSTM layer.\n",
    "\n",
    "# 4. **Dense Layer with Sigmoid Activation**:\n",
    "#    - The `Dense` layer is a fully connected layer where each neuron is connected to every neuron in the previous layer.\n",
    "#    - `1` specifies the number of neurons in the layer, as we are dealing with binary classification (positive or negative sentiment), so we have only one output neuron.\n",
    "#    - `activation='sigmoid'` specifies the activation function for the output neuron. Sigmoid activation squashes the output to the range [0, 1], which is suitable for binary classification where the output represents the probability of the positive class.\n",
    "\n",
    "# In summary, this architecture takes tokenized and padded sequences of words, processes them through an embedding layer, applies dropout for regularization, passes the sequences through an LSTM layer to capture sequential information, and finally, produces a binary sentiment prediction using a dense layer with a sigmoid activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcfdd30d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
